{"block_file": {"custom/dataproc_test_custom.py:custom:python:dataproc test custom": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/dataproc_test_custom.py", "language": "python", "type": "custom", "uuid": "dataproc_test_custom"}, "custom/gh_get_data.py:custom:python:gh get data": {"content": "import os\nimport requests\nfrom datetime import timedelta\nimport shutil\n# from google.cloud import storage\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n\n    #by default loading date is yesterday\n    load_date =(kwargs['execution_date'] - timedelta(days=1) ) \\\n        .strftime(\"%Y-%m-%d\")\n\n    #overwrite load_date if the value passed in p_load_date variable (YYYY-MM-DD)\n    p_load_date = kwargs['p_load_date']\n    if p_load_date != '':\n        load_date = p_load_date         \n\n    print(f'load_date = {load_date}')\n\n    output_dir = f\"data_{load_date}\" \n\n     # Cleanup output dir if it exists\n    shutil.rmtree(output_dir,  ignore_errors=True)\n\n    # Create the directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)   \n\n    for h in range(0,24):\n        file_name = f\"{load_date}-{h}.json.gz\"\n        url = f\"https://data.gharchive.org/{file_name}\"\n        output_file = file_name\n         # Full path of the output file\n        output_path = os.path.join(output_dir, output_file)\n        print(url)\n\n        # Send a GET request to the URL\n        response = requests.get(url)\n\n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            # Open a file in binary write mode\n            with open(output_path, \"wb\") as file:\n                # Write the content of the response to the file\n                file.write(response.content)\n            print(\"File downloaded successfully.\")\n        else:\n            print(f\"Error downloading the file. Status code: {response.status_code}\")\n            \n    \n    return output_dir  \n\n    \n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/gh_get_data.py", "language": "python", "type": "custom", "uuid": "gh_get_data"}, "custom/gh_spark_transformation.py:custom:python:gh spark transformation": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef start_cluster(cluster_client,project_id,region,cluster_name):\n    print('Starting cluster...')\n    operation = cluster_client.start_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )        \n    operation.result()\n    # print(operation.result())\n\ndef stop_cluster(cluster_client,project_id,region,cluster_name):\n    print('Stopping cluster...')\n    operation = cluster_client.stop_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )        \n    operation.result()\n    # print(operation.result())\n\ndef is_running_cluster(cluster_client,project_id,region,cluster_name):\n    cluster_request = cluster_client.get_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )\n    return cluster_request.status.state == 2    \n\n@custom\ndef load_data(*args, **kwargs):\n    from google.cloud import dataproc_v1 as dataproc   \n    import time\n    import os\n    from datetime import timedelta\n\n    # os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/src/keys/mage-runner-creds.json\"\n\n    # input_path = 'gs://de-zoomcamp-shamdzmi-bucket/data/raw/2024-03-27/*.json.gz'\n    # output_path = 'gs://de-zoomcamp-shamdzmi-bucket/data/stage/2024-03-27/'\n\n    # gs://de-zoomcamp-shamdzmi-bucket/Code/githunb_transform_raw_stage.py\n    # --input_path=gs://de-zoomcamp-shamdzmi-bucket/data/raw/2024-03-27/*.json.gz\n    # --output_path=gs://de-zoomcamp-shamdzmi-bucket/data/stage/2024-03-27/\n\n\n \n\n    # Define your cluster details\n    project_id = kwargs['p_project_id']\n    region = kwargs['p_region']\n    cluster_name = kwargs['p_cluster_name']\n    flag_stop_cluster=kwargs['p_flag_stop_cluster'] \n    pyspark_file = kwargs['p_pyspark_file']\n    gcs_bucket_name = kwargs['p_gcs_bucket_name']\n    stage_path = kwargs['p_gcs_stage_path']\n    raw_path = kwargs['p_gcs_raw_path']    \n    \n\n    #by default loading date is yesterday\n    load_date =(kwargs['execution_date'] - timedelta(days=1) ) \\\n        .strftime(\"%Y-%m-%d\")\n\n    #overwrite load_date if the value passed in p_load_date variable (YYYY-MM-DD)\n    p_load_date = kwargs['p_load_date']\n    if p_load_date != '':\n        load_date = p_load_date      \n\n    print(load_date)       \n\n    input_path = f\"gs://{gcs_bucket_name}/{raw_path}{load_date.split('-')[0]}/{load_date.split('-')[1]}/{load_date.split('-')[2]}/*.json.gz\"\n    output_path = f\"gs://{gcs_bucket_name}/{stage_path}\"    \n\n    cluster_client = dataproc.ClusterControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format(region)\n    })\n\n\n    #Check the state of the Dataproc cluster and Start if not running\n    if not is_running_cluster(cluster_client,project_id,region,cluster_name):\n        print('Cluster is not in a Running state')\n        start_cluster(cluster_client,project_id,region,cluster_name)\n\n        # Initialize a Dataproc instance\n    client = dataproc.JobControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format(region)\n    }) \n\n\n    # Prepare  pyspark job details\n    job_payload = {\n        'placement': {\n            'cluster_name': cluster_name\n        },\n        'pyspark_job': {\n            'main_python_file_uri': pyspark_file,\n            \"args\":[\n                f\"--input_path={input_path}\",\n                f\"--output_path={output_path}\",\n                f\"--load_date={load_date}\"\n            ]\n        }\n    }\n\n    # Submit the job\n    job_response = client.submit_job(project_id=project_id, region=region, job=job_payload)\n\n    # Output a response\n    print('Submitted job ID {}'.format(job_response.reference.job_id))\n   \n    job_id = job_response.reference.job_id\n    job_info = client.get_job(project_id=project_id, region=region, job_id = job_id)\n    is_done = job_info.done\n    state =  job_info.status.state\n    while not is_done:\n        time.sleep(3)\n        job_info = client.get_job(project_id=project_id, region=region, job_id = job_id)\n        state =  job_info.status.state\n        is_done = job_info.done\n        print(f\"Job state: {state} , is_done = {is_done}\")\n\n    \n    #  if the flag is set then stop Dataproc cluster\n    if flag_stop_cluster != 'N':\n        stop_cluster(cluster_client,project_id,region,cluster_name)\n    \n    return state\n\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert output == 5, 'Job failed, pls check logs in DataProc'\n\n", "file_path": "custom/gh_spark_transformation.py", "language": "python", "type": "custom", "uuid": "gh_spark_transformation"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/nameless_wind.py:data_exporter:python:nameless wind": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    # Specify your data exporting logic here\n\n\n", "file_path": "data_exporters/nameless_wind.py", "language": "python", "type": "data_exporter", "uuid": "nameless_wind"}, "data_exporters/gh_load_files_to_gcs.py:data_exporter:python:gh load files to gcs": {"content": "import os\nimport shutil\nfrom google.cloud import storage\nfrom datetime import timedelta\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports files to GCS bucket\n\n    Args:\n        data: The output from the upstream parent block. Expected the name of the directory with downloaded files.\n        args: The output from any additional upstream blocks (if applicable)    \n    \"\"\"\n    # Specify your data exporting logic here\n\n    print(f'data= {data}')\n    print(f\"exec_date = {kwargs['execution_date']}\")\n    print(f\"p_load_date = {kwargs['p_load_date']}\")\n\n    #by default loading date is yesterday\n    load_date =(kwargs['execution_date'] - timedelta(days=1) ) \\\n        .strftime(\"%Y-%m-%d\")\n\n    #overwrite load_date if the value passed in p_load_date variable (YYYY-MM-DD)\n    p_load_date = kwargs['p_load_date']\n    if p_load_date != '':\n        load_date = p_load_date         \n\n    print(f'load_date = {load_date}')\n\n    input_dir = f\"data_{load_date}\"    \n    if data != '':\n        input_dir = data\n\n    print(input_dir)\n\n    #    Create a GCS client\n    client = storage.Client()    \n    \n    bucket = client.bucket(kwargs['p_gcs_bucket_name'])    \n    print ('Bucket name: ',kwargs['p_gcs_bucket_name'])\n    raw_prefix = f\"{kwargs['p_gcs_raw_path']}{load_date.split('-')[0]}/{load_date.split('-')[1]}/{load_date.split('-')[2]}\"\n    \n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            blob_path = raw_prefix+'/'+os.path.relpath(file_path, input_dir)            \n            blob = bucket.blob(blob_path)\n            blob.upload_from_filename(file_path)\n            print(f\"Uploaded {blob_path} to GCS.\")\n\n    # Cleanup output dir\n    shutil.rmtree(input_dir)\n    \n\n    \n     \n\n\n", "file_path": "data_exporters/gh_load_files_to_gcs.py", "language": "python", "type": "data_exporter", "uuid": "gh_load_files_to_gcs"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/test_bigquery.sql:data_loader:sql:test bigquery": {"content": "-- Docs: https://docs.mage.ai/guides/sql-blocks\n-- select f from test2323;\n-- select year,month,day, count(*) from de_zoomcamp_dataset.test_repo_ex\n-- group by year,month,day\n\n-- select '{{ variables(\"test_var\") }}' \n-- MERGE INTO de_zoomcamp_dataset.dim_repo tgt\n-- USING\n-- (\n--   select id as repo_id , name as repo_name, url as repo_url from de_zoomcamp_dataset.stage_repo_external\n--   where \n--   year = 2024\n--   and month = 3\n--   and day = 20\n-- ) src\n-- ON tgt.repo_id = src.repo_id \n-- WHEN MATCHED \n--   AND( tgt.repo_name != src.repo_name \n--    OR tgt.repo_url != src.repo_url)\n-- THEN UPDATE\n--   SET \n--   tgt.repo_name = src.repo_name,\n--   tgt.repo_url = src.repo_url\n-- WHEN NOT MATCHED THEN \n--  INSERT (repo_id, repo_name, repo_url)\n--  VALUES (src.repo_id, src.repo_name, src.repo_url)\n\n\n-- DROP TABLE IF EXISTS de_zoomcamp_dataset.dim_repo ;\n-- CREATE TABLE IF NOT EXISTS de_zoomcamp_dataset.dim_repo (\n--   repo_id NUMERIC(20,0) OPTIONS (description = 'Repository ID'),\n--   repo_name  STRING OPTIONS (description = 'Repository name'),\n--   repo_url  STRING OPTIONS (description = 'Repository URL')  \n-- ) \n-- CLUSTER BY\n--   repo_id\n-- OPTIONS (    \n--     description = 'Repository dimension'\n-- )\n-- ;\n\n\n", "file_path": "data_loaders/test_bigquery.sql", "language": "sql", "type": "data_loader", "uuid": "test_bigquery"}, "data_loaders/dataproc_test.py:data_loader:python:dataproc test": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef start_cluster(cluster_client,project_id,region,cluster_name):\n    print('Starting cluster...')\n    operation = cluster_client.start_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )        \n    # print(operation.result())\n\ndef stop_cluster(cluster_client,project_id,region,cluster_name):\n    print('Stopping cluster...')\n    operation = cluster_client.stop_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )        \n    # print(operation.result())\n\ndef is_running_cluster(cluster_client,project_id,region,cluster_name):\n    cluster_request = cluster_client.get_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )\n    return cluster_request.status.state == 2    \n\n@data_loader\ndef load_data(*args, **kwargs):\n    from google.cloud import dataproc_v1 as dataproc   \n    import time\n    import os\n\n    # os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/src/keys/mage-runner-creds.json\"\n\n    # input_path = 'gs://de-zoomcamp-shamdzmi-bucket/data/raw/2024-03-27/*.json.gz'\n    # output_path = 'gs://de-zoomcamp-shamdzmi-bucket/data/stage/2024-03-27/'\n\n    # gs://de-zoomcamp-shamdzmi-bucket/Code/githunb_transform_raw_stage.py\n    # --input_path=gs://de-zoomcamp-shamdzmi-bucket/data/raw/2024-03-27/*.json.gz\n    # --output_path=gs://de-zoomcamp-shamdzmi-bucket/data/stage/2024-03-27/\n\n\n    cluster_client = dataproc.ClusterControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format('europe-west1')\n    })\n\n    # Define your cluster details\n    project_id = 'de-zoomcamp-shamdzmi'\n    region = 'europe-west1'\n    cluster_name = 'dpc-zoomcamp'\n    flag_stop_cluster=True \n    pyspark_file = 'gs://de-zoomcamp-shamdzmi-bucket/Code/githunb_transform_raw_stage.py'\n\n\n    #Check the state of the Dataproc cluster and Start if not running\n    if not is_running_cluster(cluster_client,project_id,region,cluster_name):\n        print('Cluster is not in a Running state')\n        start_cluster(cluster_client,project_id,region,cluster_name)\n\n        # Initialize a Dataproc instance\n    client = dataproc.JobControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format('europe-west1')\n    }) \n\n\n    # Prepare  pyspark job details\n    job_payload = {\n        'placement': {\n            'cluster_name': cluster_name\n        },\n        'pyspark_job': {\n            'main_python_file_uri': pyspark_file,\n            \"args\":[\n                \"--input_path=gs://de-zoomcamp-shamdzmi-bucket/data/raw/2024-03-27/*.json.gz\",\n                \"--output_path=gs://de-zoomcamp-shamdzmi-bucket/data/stage/2024-03-27/\"\n            ]\n        }\n    }\n\n    # \"pysparkJob\": {\n    #   \"mainPythonFileUri\": \"gs://de-zoomcamp-shamdzmi-bucket/Code/githunb_transform_raw_stage.py\",\n    #   \"properties\": {},\n    #   \"args\": [\n    #     \"--sd=aa\"\n    #   ]\n    # }\n\n    # Submit the job\n    job_response = client.submit_job(project_id=project_id, region=region, job=job_payload)\n\n    # Output a response\n    print('Submitted job ID {}'.format(job_response.reference.job_id))\n   \n    job_id = job_response.reference.job_id\n    job_info = client.get_job(project_id=project_id, region=region, job_id = job_id)\n    is_done = job_info.done\n    state =  job_info.status.state\n    while not is_done:\n        time.sleep(3)\n        job_info = client.get_job(project_id=project_id, region=region, job_id = job_id)\n        state =  job_info.status.state\n        is_done = job_info.done\n        print(f\"Job state: {state} , is_done = {is_done}\")\n\n    \n    #  if the flag is set then stop Dataproc cluster\n    if flag_stop_cluster:\n        stop_cluster(cluster_client,project_id,region,cluster_name)\n    \n    return state\n\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert output == 5, 'Job failed, pls check logs in DataProc'\n", "file_path": "data_loaders/dataproc_test.py", "language": "python", "type": "data_loader", "uuid": "dataproc_test"}, "data_loaders/gcs_download.py:data_loader:python:gcs download": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'    \n\n    bucket_name = 'de-zoomcamp-shamdzmi-bucket'\n    object_key = 'titanic.csv'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/gcs_download.py", "language": "python", "type": "data_loader", "uuid": "gcs_download"}, "data_loaders/gh_download_data.py:data_loader:python:gh download data": {"content": "\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    \n    url = 'https://data.gharchive.org/2015-01-01-15.json.gz'\n    file_name = '/data/2015-01-01-15.json.gz'\n\n    urlretrieve(url, file_name)\n\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/gh_download_data.py", "language": "python", "type": "data_loader", "uuid": "gh_download_data"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_age = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_age)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/load_fct_events.sql:transformer:sql:load fct events": {"content": "--  Delete partition for load date\n DELETE FROM \n    de_zoomcamp_dataset.fct_events  t  \n WHERE \n    TIMESTAMP_TRUNC(event_ts, DAY) IN \n    (\n    SELECT \n        CAST(load_dt as TIMESTAMP) AS l_ts\n    FROM\n    (\n        SELECT    \n            CASE \n                WHEN '{{ variables(\"p_load_date\") }}' = '' \n                -- if parameter is not defined, then take day previous to execution date\n                    THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n            END\n        AS load_dt\n    ));\n\n\n\n--INSERT DATA INTO PARTITION FOR LOAD DATE\nINSERT INTO de_zoomcamp_dataset.fct_events \n(\n  event_id,\n  event_ts,\n  actor_id,\n  org_id,\n  repo_id,\n  public,\n  type \n)\nWITH cte_load_date as \n( -- get load date\n    SELECT \n        EXTRACT(YEAR FROM load_dt) AS l_year\n        ,EXTRACT(MONTH FROM load_dt) AS l_month\n        ,EXTRACT(DAY FROM load_dt) AS l_day        \n    FROM\n    (\n        SELECT    \n            CASE \n                WHEN '{{ variables(\"p_load_date\") }}' = '' \n                -- if parameter is not defined, then take day previous to execution date\n                    THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n            END\n        AS load_dt\n    )\n)  \nSELECT\n CAST(id AS NUMERIC ) AS event_id,\n CAST(created_at AS TIMESTAMP) AS event_ts,\n CAST(actor_id AS NUMERIC) AS actor_id,\n CAST(org_id AS NUMERIC) AS org_id ,\n CAST(repo_id AS NUMERIC) AS repo_id,\n CAST(public AS BOOL) AS public,\n type \nFROM\n    de_zoomcamp_dataset.stage_events_external t\n    CROSS JOIN cte_load_date l\nWHERE \n    t.year = l.l_year\n    AND t.month = l.l_month\n    AND t.day = l.l_day;\n\n\n", "file_path": "transformers/load_fct_events.sql", "language": "sql", "type": "transformer", "uuid": "load_fct_events"}, "transformers/gh_trasform_raw_data.py:transformer:python:gh trasform raw data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/gh_trasform_raw_data.py", "language": "python", "type": "transformer", "uuid": "gh_trasform_raw_data"}, "transformers/load_fct_events_dd.sql:transformer:sql:load fct events dd": {"content": " --DELETE DATA FROM PARTITION\n DELETE FROM \n    de_zoomcamp_dataset.fct_events_dd t  \n WHERE \n    event_date IN  (\n        SELECT    \n            CASE \n                WHEN '{{ variables(\"p_load_date\") }}' = '' \n                -- if parameter is not defined, then take day previous to execution date\n                    THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n            END\n        AS load_dt \n    );\n\n\n--INSERT DATA INTO PARTITION FOR LOAD DATE\nINSERT INTO  de_zoomcamp_dataset.fct_events_dd\n(\n event_date,\n actor_login,\n actor_url, \n org_login,\n org_url,\n repo_name,\n repo_url,\n event_type,\n public,\n count\n)\nWITH cte_load_date as \n( -- get load date\n    SELECT \n        CAST(load_dt as TIMESTAMP) AS l_ts\n    FROM\n    (\n        SELECT    \n            CASE \n                WHEN '{{ variables(\"p_load_date\") }}' = '' \n                -- if parameter is not defined, then take day previous to execution date\n                    THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n            END\n        AS load_dt\n    )\n)   \nSELECT \n CAST(TIMESTAMP_TRUNC(f.event_ts, DAY) AS DATE) as event_date,\n a.actor_login,\n a.actor_url, \n o.org_login,\n o.org_url,\n r.repo_name,\n r.repo_url,\n type as event_type,\n public,\n COUNT(*) as count\nFROM \n  de_zoomcamp_dataset.fct_events f \n  LEFT JOIN de_zoomcamp_dataset.dim_actor a on f.actor_id = a.actor_id\n  LEFT JOIN de_zoomcamp_dataset.dim_org o on f.org_id = o.org_id\n  LEFT JOIN de_zoomcamp_dataset.dim_repo r on f.repo_id = r.repo_id  \nWHERE \n  TIMESTAMP_TRUNC(f.event_ts, DAY) IN (SELECT l_ts FROM cte_load_date) \nGROUP BY CAST(TIMESTAMP_TRUNC(f.event_ts, DAY) AS DATE),\n a.actor_login,\n a.actor_url, \n o.org_login,\n o.org_url,\n r.repo_name,\n r.repo_url,\n type,\n public\n;", "file_path": "transformers/load_fct_events_dd.sql", "language": "sql", "type": "transformer", "uuid": "load_fct_events_dd"}, "transformers/gh_create_bq_tables.sql:transformer:sql:gh create bq tables": {"content": "--  CREATES TABLES IN  BIGQUERY IF THEY DON'T EXIST YET\n\n--  CREATE External tables based on Stage Parquet files. Tables are partitioned by year/month/day, according to the folder structure in GCS\n\nCREATE EXTERNAL TABLE IF NOT EXISTS de_zoomcamp_dataset.stage_repo_external\nWITH PARTITION COLUMNS (\nyear INT64, \nmonth INT64,\nday INT64)\nOPTIONS (\nuris = ['gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}repo/*'],\nformat = 'PARQUET',\nhive_partition_uri_prefix = 'gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}repo/',\nrequire_hive_partition_filter = false);\n\n\nCREATE EXTERNAL TABLE IF NOT EXISTS de_zoomcamp_dataset.stage_org_external\nWITH PARTITION COLUMNS (\nyear INT64, \nmonth INT64,\nday INT64)\nOPTIONS (\nuris = ['gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}org/*'],\nformat = 'PARQUET',\nhive_partition_uri_prefix = 'gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}org/',\nrequire_hive_partition_filter = false);\n\n\nCREATE EXTERNAL TABLE IF NOT EXISTS de_zoomcamp_dataset.stage_actor_external\nWITH PARTITION COLUMNS (\nyear INT64, \nmonth INT64,\nday INT64)\nOPTIONS (\nuris = ['gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}actor/*'],\nformat = 'PARQUET',\nhive_partition_uri_prefix = 'gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}actor/',\nrequire_hive_partition_filter = false);\n\nCREATE EXTERNAL TABLE IF NOT EXISTS de_zoomcamp_dataset.stage_events_external\nWITH PARTITION COLUMNS (\nyear INT64, \nmonth INT64,\nday INT64)\nOPTIONS (\nuris = ['gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}events/*'],\nformat = 'PARQUET',\nhive_partition_uri_prefix = 'gs://{{ variables(\"p_gcs_bucket_name\") }}/{{ variables(\"p_gcs_stage_path\") }}events/',\nrequire_hive_partition_filter = false);\n\n\n\n\n-- CREATE DIMENSIONS (BQ Standart tables)\n\n-- DROP TABLE IF EXISTS de_zoomcamp_dataset.dim_repo ;\nCREATE TABLE IF NOT EXISTS de_zoomcamp_dataset.dim_repo (\n  repo_id NUMERIC(20,0) OPTIONS (description = 'Repository ID'),\n  repo_name  STRING OPTIONS (description = 'Repository name'),\n  repo_url  STRING OPTIONS (description = 'Repository URL')  \n) \nCLUSTER BY\n  repo_id\nOPTIONS (    \n    description = 'Repository dimension'\n)\n;\n\n\n-- DROP TABLE IF EXISTS de_zoomcamp_dataset.dim_actor ;\nCREATE TABLE IF NOT EXISTS de_zoomcamp_dataset.dim_actor  (\n  actor_id NUMERIC(20,0) OPTIONS (description = 'Actor ID'),\n  actor_login  STRING OPTIONS (description = 'Actor login'),\n  actor_url  STRING OPTIONS (description = 'Actor URL')  \n) \nCLUSTER BY\n  actor_id\nOPTIONS (    \n    description = 'Actor dimension'\n)\n;\n\n-- DROP TABLE IF EXISTS de_zoomcamp_dataset.dim_org ;\nCREATE TABLE IF NOT EXISTS de_zoomcamp_dataset.dim_org  (\n  org_id NUMERIC(20,0) OPTIONS (description = 'Organization ID'),\n  org_login  STRING OPTIONS (description = 'Organization login'),\n  org_url  STRING OPTIONS (description = 'Organization URL')  \n) \nCLUSTER BY\n  org_id\nOPTIONS (    \n    description = 'Organization dimension'\n)\n;\n\n\n-- CREATE FACT TABLES (BQ Standart tables)\n\n-- Daily aggregated events fact table (joined with dimensions)\n-- DROP TABLE IF EXISTS de_zoomcamp_dataset.fct_events_dd ;\nCREATE TABLE IF NOT EXISTS de_zoomcamp_dataset.fct_events_dd  (  \n  event_date  DATE OPTIONS (description = 'Event date'),\n  actor_login STRING OPTIONS (description = 'actor url'),\n  actor_url STRING OPTIONS (description = 'actor url'),\n  org_login STRING OPTIONS (description = 'Org login'),\n  org_url STRING OPTIONS (description = 'Org url'),\n  repo_name STRING OPTIONS (description = 'Repo name'),\n  repo_url STRING OPTIONS (description = 'Repo url'),\n  public BOOL OPTIONS (description = 'Public flag'),\n  event_type STRING OPTIONS (description = 'Event type'),\n  count NUMERIC(20,0) OPTIONS (description = 'Count events')\n)\nPARTITION BY\n  event_date  \nOPTIONS (    \n    description = 'Events fact table aggregated on daily level'\n)\n;\n\n\n-- Granular events fact table\n-- DROP TABLE IF EXISTS de_zoomcamp_dataset.fct_events\nCREATE TABLE IF NOT EXISTS de_zoomcamp_dataset.fct_events  (\n  event_id NUMERIC(20,0) OPTIONS (description = 'Event ID'),\n  event_ts  TIMESTAMP OPTIONS (description = 'Event timestamp'),\n  actor_id NUMERIC(20,0) OPTIONS (description = 'Actor ID'),\n  org_id NUMERIC(20,0) OPTIONS (description = 'Org ID'),\n  repo_id NUMERIC(20,0) OPTIONS (description = 'Repo ID'),\n  public BOOL OPTIONS (description = 'Public flag'),\n  type STRING OPTIONS (description = 'Event type')\n)\nPARTITION BY\n  TIMESTAMP_TRUNC(event_ts, DAY) \nCLUSTER BY\n  event_id\nOPTIONS (    \n    description = 'Events fact table'\n)\n;\n\n", "file_path": "transformers/gh_create_bq_tables.sql", "language": "sql", "type": "transformer", "uuid": "gh_create_bq_tables"}, "transformers/load_dim_actor.sql:transformer:sql:load dim actor": {"content": "\n\n-- --upsert actor dimension with staging data (for a given day)\n\nMERGE INTO de_zoomcamp_dataset.dim_actor tgt\nUSING\n(\n  WITH cte_load_date as \n    ( -- get load date\n        SELECT \n            EXTRACT(YEAR FROM load_dt) AS l_year\n            ,EXTRACT(MONTH FROM load_dt) AS l_month\n            ,EXTRACT(DAY FROM load_dt) AS l_day\n        FROM\n        (\n            SELECT    \n                CASE \n                    WHEN '{{ variables(\"p_load_date\") }}' = '' \n                    -- if parameter is not defined, then take day previous to execution date\n                        THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                    ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n                END\n            AS load_dt\n        )\n    )    \n    SELECT \n        t.id AS actor_id \n        ,t.login AS actor_login\n        ,t.url AS actor_url \n    FROM de_zoomcamp_dataset.stage_actor_external t\n        CROSS JOIN cte_load_date l\n    WHERE \n        t.year = l.l_year\n        AND t.month = l.l_month\n        AND t.day = l.l_day\n) src\nON tgt.actor_id = src.actor_id \nWHEN MATCHED \n  AND( tgt.actor_login != src.actor_login \n   OR tgt.actor_url != src.actor_url)\nTHEN UPDATE\n  SET \n  tgt.actor_login = src.actor_login,\n  tgt.actor_url = src.actor_url\nWHEN NOT MATCHED THEN \n INSERT (actor_id, actor_login, actor_url)\n VALUES (src.actor_id, src.actor_login, src.actor_url);  -- Docs: https://docs.mage.ai/guides/sql-blocks\n", "file_path": "transformers/load_dim_actor.sql", "language": "sql", "type": "transformer", "uuid": "load_dim_actor"}, "transformers/test_dataproc.py:transformer:python:test dataproc": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    from google.cloud import dataproc_v1 as dataproc\n\n    # Initialize a Dataproc instance\n    client = dataproc.JobControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format('europe-west1')\n    })\n\n    # Define your cluster details\n    project_id = 'de-zoomcamp-shamdzmi'\n    region = 'europe-west1'\n    cluster_name = 'dpc-zoomcamp'\n\n    # Prepare your pyspark job details\n    job_payload = {\n        'placement': {\n            'cluster_name': cluster_name\n        },\n        'pyspark_job': {\n            'main_python_file_uri': 'gs://de-zoomcamp-shamdzmi-bucket/Code/dataproc_test.py'\n        }\n    }\n\n    # Submit the job\n    job_response = client.submit_job(project_id=project_id, region=region, job=job_payload)\n\n    # Output a response\n    print('Submitted job ID {}'.format(job_response.reference.job_id))\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/test_dataproc.py", "language": "python", "type": "transformer", "uuid": "test_dataproc"}, "transformers/load_dim_repo.sql:transformer:sql:load dim repo": {"content": "\n--upsert repo dimension with staging data (for a given day)\n\nMERGE INTO de_zoomcamp_dataset.dim_repo tgt\nUSING\n(\n  WITH cte_load_date as \n    ( -- get load date\n        SELECT \n            EXTRACT(YEAR FROM load_dt) AS l_year\n            ,EXTRACT(MONTH FROM load_dt) AS l_month\n            ,EXTRACT(DAY FROM load_dt) AS l_day\n        FROM\n        (\n            SELECT    \n                CASE \n                    WHEN '{{ variables(\"p_load_date\") }}' = '' \n                    -- if parameter is not defined, then take day previous to execution date\n                        THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                    ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n                END\n            AS load_dt\n        )\n    )    \n    SELECT \n        t.id AS repo_id \n        ,t.name AS repo_name\n        ,t.url AS repo_url \n    FROM de_zoomcamp_dataset.stage_repo_external t\n        CROSS JOIN cte_load_date l\n    WHERE \n        t.year = l.l_year\n        AND t.month = l.l_month\n        AND t.day = l.l_day\n) src\nON tgt.repo_id = src.repo_id \nWHEN MATCHED \n  AND( tgt.repo_name != src.repo_name \n   OR tgt.repo_url != src.repo_url)\nTHEN UPDATE\n  SET \n  tgt.repo_name = src.repo_name,\n  tgt.repo_url = src.repo_url\nWHEN NOT MATCHED THEN \n INSERT (repo_id, repo_name, repo_url)\n VALUES (src.repo_id, src.repo_name, src.repo_url);", "file_path": "transformers/load_dim_repo.sql", "language": "sql", "type": "transformer", "uuid": "load_dim_repo"}, "transformers/ddd.sql:transformer:sql:ddd": {"content": "--  Delete partition for load date\nWITH cte_load_date as \n( -- get load date\n    SELECT \n        CAST(load_dt as TIMESTAMP) AS l_ts\n    FROM\n    (\n        SELECT    \n            CASE \n                WHEN '{{ variables(\"p_load_date\") }}' = '' \n                -- if parameter is not defined, then take day previous to execution date\n                    THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n            END\n        AS load_dt\n    )\n)  \n DELETE FROM \n    de_zoomcamp_dataset.fct_events  t  \n WHERE \n    TIMESTAMP_TRUNC(event_ts, DAY) IN  (SELECT l_ts FROM cte_load_date)\n\n\n\n\n-- --INSERT DATA INTO PARTITION FOR LOAD DATE\n-- INSERT INTO de_zoomcamp_dataset.fct_events \n-- (\n--   event_id,\n--   event_ts,\n--   actor_id,\n--   org_id,\n--   repo_id,\n--   public,\n--   type \n-- )\n-- WITH cte_load_date as \n-- ( -- get load date\n--     SELECT \n--         EXTRACT(YEAR FROM load_dt) AS l_year\n--         ,EXTRACT(MONTH FROM load_dt) AS l_month\n--         ,EXTRACT(DAY FROM load_dt) AS l_day        \n--     FROM\n--     (\n--         SELECT    \n--             CASE \n--                 WHEN '{{ variables(\"p_load_date\") }}' = '' \n--                 -- if parameter is not defined, then take day previous to execution date\n--                     THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n--                 ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n--             END\n--         AS load_dt\n--     )\n-- )  \n-- SELECT\n--  CAST(id AS NUMERIC ) AS event_id,\n--  CAST(created_at AS TIMESTAMP) AS event_ts,\n--  CAST(actor_id AS NUMERIC) AS actor_id,\n--  CAST(org_id AS NUMERIC) AS org_id ,\n--  CAST(repo_id AS NUMERIC) AS repo_id,\n--  CAST(public AS BOOL) AS public,\n--  type \n-- FROM\n--     de_zoomcamp_dataset.stage_events_external t\n--     CROSS JOIN cte_load_date l\n-- WHERE \n--     t.year = l.l_year\n--     AND t.month = l.l_month\n--     AND t.day = l.l_day;\n\n\n", "file_path": "transformers/ddd.sql", "language": "sql", "type": "transformer", "uuid": "ddd"}, "transformers/carefree_labyrinth.sql:transformer:sql:carefree labyrinth": {"content": "WITH cte_load_date as \n( -- get load date\n    SELECT \n        CAST(load_dt as TIMESTAMP) AS l_ts\n    FROM\n    (\n        SELECT    \n            CASE \n                WHEN '{{ variables(\"p_load_date\") }}' = '' \n                -- if parameter is not defined, then take day previous to execution date\n                    THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n            END\n        AS load_dt\n    )\n)\nselect * from cte_load_date-- Docs: https://docs.mage.ai/guides/sql-blocks\n", "file_path": "transformers/carefree_labyrinth.sql", "language": "sql", "type": "transformer", "uuid": "carefree_labyrinth"}, "transformers/load_dim_org.sql:transformer:sql:load dim org": {"content": "\n\n-- --upsert org dimension with staging data (for a given day)\n\nMERGE INTO de_zoomcamp_dataset.dim_org tgt\nUSING\n(\n  WITH cte_load_date as \n    ( -- get load date\n        SELECT \n            EXTRACT(YEAR FROM load_dt) AS l_year\n            ,EXTRACT(MONTH FROM load_dt) AS l_month\n            ,EXTRACT(DAY FROM load_dt) AS l_day\n        FROM\n        (\n            SELECT    \n                CASE \n                    WHEN '{{ variables(\"p_load_date\") }}' = '' \n                    -- if parameter is not defined, then take day previous to execution date\n                        THEN DATE_ADD(CAST(SUBSTR('{{ execution_date }}',0,10) AS DATE), INTERVAL -1 DAY)                \n                    ELSE CAST('{{ variables(\"p_load_date\") }}' AS DATE)\n                END\n            AS load_dt\n        )\n    )    \n    SELECT \n        t.id AS org_id \n        ,t.login AS org_login\n        ,t.url AS org_url \n    FROM de_zoomcamp_dataset.stage_org_external t\n        CROSS JOIN cte_load_date l\n    WHERE \n        t.year = l.l_year\n        AND t.month = l.l_month\n        AND t.day = l.l_day\n) src\nON tgt.org_id = src.org_id \nWHEN MATCHED \n  AND( tgt.org_login != src.org_login \n   OR tgt.org_url != src.org_url)\nTHEN UPDATE\n  SET \n  tgt.org_login = src.org_login,\n  tgt.org_url = src.org_url\nWHEN NOT MATCHED THEN \n INSERT (org_id, org_login, org_url)\n VALUES (src.org_id, src.org_login, src.org_url);  ", "file_path": "transformers/load_dim_org.sql", "language": "sql", "type": "transformer", "uuid": "load_dim_org"}, "/home/src/de-zoomcamp-prj/data_exporters/gh_load_files_to_gcs.py:data_exporter:python:home/src/de-zoomcamp-prj/data exporters/gh load files to gcs": {"content": "import os\nimport shutil\nfrom google.cloud import storage\nfrom datetime import timedelta\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports files to GCS bucket\n\n    Args:\n        data: The output from the upstream parent block. Expected the name of the directory with downloaded files.\n        args: The output from any additional upstream blocks (if applicable)    \n    \"\"\"\n    # Specify your data exporting logic here\n\n    print(f'data= {data}')\n    print(f\"exec_date = {kwargs['execution_date']}\")\n    print(f\"p_load_date = {kwargs['p_load_date']}\")\n\n    #by default loading date is yesterday\n    load_date =(kwargs['execution_date'] - timedelta(days=1) ) \\\n        .strftime(\"%Y-%m-%d\")\n\n    #overwrite load_date if the value passed in p_load_date variable (YYYY-MM-DD)\n    p_load_date = kwargs['p_load_date']\n    if p_load_date != '':\n        load_date = p_load_date         \n\n    print(f'load_date = {load_date}')\n\n    input_dir = f\"data_{load_date}\"    \n    if data != '':\n        input_dir = data\n\n    print(input_dir)\n\n    #    Create a GCS client\n    client = storage.Client()    \n    \n    bucket = client.bucket(kwargs['p_gcs_bucket_name'])    \n    print ('Bucket name: ',kwargs['p_gcs_bucket_name'])\n    raw_prefix = f\"{kwargs['p_gcs_raw_path']}{load_date.split('-')[0]}/{load_date.split('-')[1]}/{load_date.split('-')[2]}\"\n    \n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            blob_path = raw_prefix+'/'+os.path.relpath(file_path, input_dir)            \n            blob = bucket.blob(blob_path)\n            blob.upload_from_filename(file_path)\n            print(f\"Uploaded {blob_path} to GCS.\")\n\n    # Cleanup output dir\n    shutil.rmtree(input_dir)  \n     \n\n\n", "file_path": "/home/src/de-zoomcamp-prj/data_exporters/gh_load_files_to_gcs.py", "language": "python", "type": "data_exporter", "uuid": "gh_load_files_to_gcs"}, "/home/src/de-zoomcamp-prj/custom/gh_spark_transformation.py:custom:python:home/src/de-zoomcamp-prj/custom/gh spark transformation": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef start_cluster(cluster_client,project_id,region,cluster_name):\n    print('Starting cluster...')\n    operation = cluster_client.start_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )        \n    operation.result()\n    # print(operation.result())\n\ndef stop_cluster(cluster_client,project_id,region,cluster_name):\n    print('Stopping cluster...')\n    operation = cluster_client.stop_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )        \n    operation.result()\n    # print(operation.result())\n\ndef is_running_cluster(cluster_client,project_id,region,cluster_name):\n    cluster_request = cluster_client.get_cluster(\n    request={\"project_id\": project_id, \"region\": region, \"cluster_name\": cluster_name}\n    )\n    return cluster_request.status.state == 2    \n\n@custom\ndef load_data(*args, **kwargs):\n    from google.cloud import dataproc_v1 as dataproc   \n    import time\n    import os\n    from datetime import timedelta\n    from google.cloud import storage \n \n\n    # Define your cluster details\n    project_id = kwargs['p_project_id']\n    region = kwargs['p_region']\n    cluster_name = kwargs['p_cluster_name']\n    flag_stop_cluster=kwargs['p_flag_stop_cluster'] \n    pyspark_file = kwargs['p_pyspark_file']\n    gcs_bucket_name = kwargs['p_gcs_bucket_name']\n    stage_path = kwargs['p_gcs_stage_path']\n    raw_path = kwargs['p_gcs_raw_path']  \n    \n\n    # upload latest version of pyspark file for DataProc job into GCS\n    local_pyspark_file='de-zoomcamp-prj/pyspark_etl/githunb_transform_raw_stage.py'\n    client = storage.Client()\n    bucket = client.bucket(gcs_bucket_name)\n    blob = bucket.blob(pyspark_file.replace(f'gs://{gcs_bucket_name}/',''))\n    blob.upload_from_filename(local_pyspark_file)\n        \n\n    #by default loading date is yesterday\n    load_date =(kwargs['execution_date'] - timedelta(days=1) ) \\\n        .strftime(\"%Y-%m-%d\")\n\n    #overwrite load_date if the value passed in p_load_date variable (YYYY-MM-DD)\n    p_load_date = kwargs['p_load_date']\n    if p_load_date != '':\n        load_date = p_load_date      \n\n    print(load_date)       \n\n    input_path = f\"gs://{gcs_bucket_name}/{raw_path}{load_date.split('-')[0]}/{load_date.split('-')[1]}/{load_date.split('-')[2]}/*.json.gz\"\n    output_path = f\"gs://{gcs_bucket_name}/{stage_path}\"    \n\n    cluster_client = dataproc.ClusterControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format(region)\n    })\n\n\n    #Check the state of the Dataproc cluster and Start if not running\n    if not is_running_cluster(cluster_client,project_id,region,cluster_name):\n        print('Cluster is not in a Running state')\n        start_cluster(cluster_client,project_id,region,cluster_name)\n\n        # Initialize a Dataproc instance\n    client = dataproc.JobControllerClient(client_options={\n        'api_endpoint': '{}-dataproc.googleapis.com:443'.format(region)\n    }) \n\n\n    # Prepare  pyspark job details\n    job_payload = {\n        'placement': {\n            'cluster_name': cluster_name\n        },\n        'pyspark_job': {\n            'main_python_file_uri': pyspark_file,\n            \"args\":[\n                f\"--input_path={input_path}\",\n                f\"--output_path={output_path}\",\n                f\"--load_date={load_date}\"\n            ]\n        }\n    }\n\n    # Submit the job\n    job_response = client.submit_job(project_id=project_id, region=region, job=job_payload)\n\n    # Output a response\n    print('Submitted job ID {}'.format(job_response.reference.job_id))\n   \n    job_id = job_response.reference.job_id\n    job_info = client.get_job(project_id=project_id, region=region, job_id = job_id)\n    is_done = job_info.done\n    state =  job_info.status.state\n    while not is_done:\n        time.sleep(3)\n        job_info = client.get_job(project_id=project_id, region=region, job_id = job_id)\n        state =  job_info.status.state\n        is_done = job_info.done\n        print(f\"Job state: {state} , is_done = {is_done}\")\n\n    \n    #  if the flag is set then stop Dataproc cluster\n    if flag_stop_cluster != 'N':\n        stop_cluster(cluster_client,project_id,region,cluster_name)\n    \n    return state\n\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert output == 5, 'Job failed, pls check logs in DataProc'\n\n", "file_path": "/home/src/de-zoomcamp-prj/custom/gh_spark_transformation.py", "language": "python", "type": "custom", "uuid": "gh_spark_transformation"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}